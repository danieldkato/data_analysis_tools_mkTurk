{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e85bcb-16f0-43d0-a94f-3da330d19317",
   "metadata": {},
   "source": [
    "# Working with HDF5\n",
    "This notebook illustrates how to work with MkTurk PSTH data in HDF5 format. HDF5s allow you to slice data stored on disk, precluding the need to load all of the data from a given session before working with it. In applications where only a subset of the data are needed (e.g., only trials from a certain scenefile), this can reduce read times several times over, especially if the selected slices are contiguous on disk.\n",
    "\n",
    "By way of preliminaries, let's import some modules and do some environment setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6e039a9-c6b9-4908-afff-311f780d4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements:\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from data_analysis_tools_mkTurk.utils_meta import get_recording_path\n",
    "from data_analysis_tools_mkTurk.general import df_2_psth_mat\n",
    "from data_analysis_tools_mkTurk.IO import ch_dicts_2_h5, h5_2_trial_df, h5_2_df\n",
    "import socket\n",
    "\n",
    "# Define absolute paths to input directories depending on which machine code is running on:  \n",
    "hostname = socket.gethostname()\n",
    "if 'rc.zi.columbia.edu' in hostname:\n",
    "    mnt_path = os.path.join('/', 'mnt', 'smb', 'locker', 'issa-locker') \n",
    "else:\n",
    "    mnt_path = 'X:' # Edit this to whatever prefix is used to access Engram as a share/network drive on the local system \n",
    "    mnt_path += os.path.sep \n",
    "base_data_path = os.path.join(mnt_path, 'Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fff41-856c-4e8c-8ed7-3edb9c8d4b5b",
   "metadata": {},
   "source": [
    "      \n",
    "<br/>\n",
    "\n",
    "## 1. Writing data to HDF5\n",
    "In this first part, we'll convert pickeld dicts of trial parameters and spike count data into HDF5 format, which we'll then read from in the subsequent section.   \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "<u>**Prerequisites**</u><br/>\n",
    "This write portion of the pipeline assumes that ephys data from SpikeGLX and behavioral data from MkTurk have already been preprocessed and registered to each other using the code illustrated in notebooks `get_data_dict_from_mkturk.ipynb`, `Demo.ipynb`, `Demo_2.ipynb`, and `Demo3.ipynb`. This should result in the following files for any given session:\n",
    "\n",
    "- `data_dict_<session>`, where `<session>` stands for the name of the directory where the raw data for the session are saved.\n",
    "- `stim_info_sess`, a pickled dict of general information about the stimuli shown in the session.\n",
    "- `ch<iii>_psth_stim_meta`, a pickled dict including timing information about the peristimulus window for each stimulus, for each channels `<iii>`. \n",
    "- `ch<iii>_psth_stim`, a pickled dict of spike count data for each channels `<iii>`.\n",
    "\n",
    "For an example of what preprocessed data for a given session should look like, see `axon.rc.zi.columbia.edu/mnt/smb/locker/issa-locker/users/Dan/ephys/West/West_20231109_R_H00_P46`. Let's use that session as an example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3b94800-909e-4b5d-be77-d7ff3d953cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/smb/locker/issa-locker/Data/West\n"
     ]
    }
   ],
   "source": [
    "monkey = 'West'\n",
    "date = '20231109'\n",
    "\n",
    "# Define output directory:\n",
    "recording_path = get_recording_path(base_data_path, monkey, date, depth=4)[0]\r",
    "preprocessed_data_path = os.path.join(mnt_path, 'users', 'Dan', 'ephys', monkey, recording_path.split(os.path.sep)[3])  \n",
    "output_directory = os.path.join(mnt_path, 'users', 'Dan', 'ephys', monkey, recording_path.split(os.path.sep)[3], 'hdf_demo', 'write')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ea57d-0b1a-4082-a3e2-6c09a60ab285",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<u>**Choosing chunk size**</u><br/>\n",
    "When working with HDF5s, data are loaded in \"chunks,\" or sets of contiguous memory addresses. The chunk size determines how many such memory addresses are loaded at once. Perhaps counterintiuitively, despite the fact that this parameter relates to read behavior, it has to be set when the file is written. **Choosing an appropriate chunk size is really important!** The difference between a good and a bad chunk size can be a >100x difference in read/write times! In the past a chunk size of 100 has worked pretty well for MkTurk data. Also, since spike counts are always integers, we can save a little time and storage space by specifying that the datatype should be `int`:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b227215c-4e10-45de-b529-f288a778af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose HDF5 params:\n",
    "chunk_size = 100\n",
    "dtype = int "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e848ed4-b005-48bf-9cd7-fe3a50e18720",
   "metadata": {},
   "source": [
    " <br/>\n",
    " <br/>\n",
    " \n",
    "<u>**Writing HDF5 to disk**</u><br/>\n",
    "We're almost ready to convert the preprocessed data to HDF5. First though, for demonstration/debugging purposes, we can also select a subset of channels to save data for. Let's just choose `n_channels` channels here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fa47091-b202-4155-88ea-ccfc44be0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose channels to save data for:\n",
    "n_channels = 384\n",
    "channels = np.arange(n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9820e08-83a9-4f53-9a76-ead0a84b2cf7",
   "metadata": {},
   "source": [
    "In this case the channels happen to be consecutively numbered, but note in principle it's possible to select any arbitrary array (or list) of channel indices. Now we're ready to convert the preprocessed data to HDF5 using the `ch_dicts_2_h5()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb0ebf3-ae98-4725-82f9-844b85d9bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\\Data\\West\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\users\\Dan\\code\\data_analysis_tools_mkTurk\\IO.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  trial_params_df['idx_merge'] = np.arange(n_rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataframe of stimulus conditions...\n",
      "X:\\Data\\West\n",
      "X:\\Data\\West\n",
      "Loading data for channel 1 of 4...\n",
      "Loading data for channel 2 of 4...\n",
      "Loading data for channel 3 of 4...\n",
      "Loading data for channel 4 of 4...\n",
      "Saving HDF5 to disk...\n",
      "... done.\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data as HDF5:\n",
    "start = time.time() \n",
    "ch_dicts_2_h5(base_data_path, monkey, date, preprocessed_data_path, channels=channels, chunk_size=chunk_size, dtype=dtype, \n",
    "    save_output=True, output_directory=output_directory)\n",
    "stop = time.time()\n",
    "print('... done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179b16e-ea69-4c6a-9b7f-c49594c0580a",
   "metadata": {},
   "source": [
    "The amount of time this step takes depends on where you're running the code/where you're writing to. If writing from Axon to Engram, saving an entire session should take under a minute; if saving from a laptop to Engram over Columbia private WiFi, about 5-12 minutes; over the public Internet (via VPN), upwards of an hour. This sounds like a lot, but the idea is that you only have to do it once (or at least rarely) and then it'll pay off in the longer run with all the time you save loading in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ca26bc-79a0-435b-bcb6-535cf4025235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write duration = 2.572272205352783 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Write duration = {} minutes'.format( (stop-start)/60 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60292d8-260e-4b51-bf4a-6446c3463055",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<u>**A caveat about time bins**</u></br>\n",
    "Note that HDF5 stores data in an array-like structure, meaning that PSTHs for all stimuli must be of uniform shape -- meaning in turn that they must all have the same number of time bins. However, also note that different stimuli within a session can be of different duration (e.g., RSVP44 vs freeview stimuli). To accommodate these differences, `ch_dicts_2_h5()` pads PSTHs for all stimuli to be equal in duration to the max PSTH duration across the whole session. In other words, the number of time bins for every stimulus is made to equal the number of time bins for the longest stimulus. Given a session where the maximum stim duration is _v_ time bins, any shorter stimulus _u_ bins in duration will be aligned to the _start_ of the peristimulus window so that only the first _u_ out of _v_ bins will include spike data. Any excess time bins will be all nan. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fe31d-b47a-4152-bb0c-335c2864a513",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "# 2. Reading data from HDF5\n",
    "Now that the preprocessed data have been saved to an HDF5, we can try reading it into RAM. \n",
    "\n",
    "<br/>\n",
    "\n",
    "<u>**Pre-selecting trials**</u><br/>\n",
    "One of the useful things about HDF5 is that we can retrieve specific data slices from disk without having to load the entire file. In practice, this is often useful for loading just a subset of trials. Suppose we only want trials from scenefiles `neural_stim_5_1ABC_00` and `neural_stim_5_2UVW_03`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23e9aa0c-cd29-4643-a08a-ed7dea36f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rc.zi.columbia.edu' in hostname:\n",
    "    folder_level_offset = 7\n",
    "else:\n",
    "    folder_level_offest = 3    \n",
    "h5_path = os.path.join(mnt_path, 'users', 'Dan', 'ephys', monkey, recording_path.split(os.path.sep)[folder_level_offset], 'hdf_demo', 'read', 'all_psth.h5')\n",
    "scenefiles = ['/mkturkfiles/scenebags/West/neural_stim_5_1ABC_00.json', \n",
    "              '/mkturkfiles/scenebags/West/neural_stim_5_2UVW_03.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b1cdd-296c-4a5c-94ab-332b0c653599",
   "metadata": {},
   "source": [
    "In order to retrive just the responses to these scenefiles, we first need to know which trials are associated with each one. To check this, we can first load _just_ the trial parameters (without the actual PSTH data) using the function `h5_2_trial_df()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3b271d0-e51a-4c69-bd7b-60c5b2715739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk2643/miniforge3/envs/mkanalysis/lib/python3.11/site-packages/tables/attributeset.py:322: DataTypeWarning: Unsupported type for attribute 'scenefile_by_stim_mat' in node '/'. Offending HDF5 class: 8\n",
      "  value = self._g_getattr(self._v_node, name)\n"
     ]
    }
   ],
   "source": [
    "# Load trial parameters without PSTH data:\n",
    "trial_params = h5_2_trial_df(h5_path, params='short')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3f5d4-f430-4254-bd7e-b3ad29222d15",
   "metadata": {},
   "source": [
    "This will return a pandas dataframe `trial_params` with columns `trial_num`, `rsvp_num`, `stim_id`, and `scenefile`. Each row corresponds to an individual stimulus presentation. The columns represent just a small subset of parameters we can use to search stimulus presentations, but we can also get a more complete table of parameters by passing `params='all'` to `h5_2_trial_df()` (although the default subset of trial parameters is sufficient for most applications). We can then search for rows (stimulus presentations) associated with our scenefiles of interest using the standard constructions for querying dataframes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95beed0b-855c-425c-90ff-1a405e0032ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stimulus presentations associated with requested scenefiles:\n",
    "bool_filt = trial_params.scenefile.isin(scenefiles)\n",
    "rsvp44_trials = trial_params[bool_filt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45eb10-2e8b-4d12-9720-c89a6b0e0724",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<u>**Fetching spike data**</u><br/>\n",
    "Now that we know which trials we want spike counts for, we can use the `h5_2_df()` function along with our dataframe of selected trials to load PSTHs just for those trials. Suppose moreover that we just want data from a certain time window and from certain channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d46ff3ad-35d7-4771-a372-f52ac6a0b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-fetching PSTHs from HDF5...\n",
      "... done.\n",
      "Duration=0.03562094767888387 minutes\n",
      "Fancy slicing numpy array...\n",
      "... done.\n",
      "Duration=0.009167007605234782 minutes\n"
     ]
    }
   ],
   "source": [
    "# Read spike count data from HDF5 for requested trials:\n",
    "time_window = [-0.1, 0.25] # Beginning and end of peristimulus time window for each stim, relative to stim onset in seconds   \n",
    "rsvp44_data = h5_2_df(h5_path, trials=rsvp44_trials, time_window=time_window, channels=channels) # Fetch PSTHs="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c0a43-9269-4fac-8695-a6c161b4cfcd",
   "metadata": {},
   "source": [
    "Note this new dataframe `rsvp44_data` includes an additional column `psth` containing actual binned spike count data. For each row, the value of `psth` is a _c_-by-_t_ numpy array of binned spike counts, where _c_ is the number of channels in the loaded data, and _t_ is the number of time bins in the requested peri-stimulus time window. <br/>\n",
    "\n",
    "Alternatively, `trials` can be an _s_-by-2 numpy array of trial/RSVP indices, where the first column consists of trial numbers and the second column consists of RSVP numbers within the corresponding trial. If `trials`, `time_window`, or `channels` are left unset, then all data for all trials, time bins, and channels will be returned, respectively.<br/>\n",
    "\n",
    "To extract just the spike data into a numpy array, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "807d3b76-acfa-4509-9b0d-1b5df8635a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spike count data as array:\n",
    "spikes = np.array(list(rsvp44_data['psth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2937df7-0ad6-47c2-b906-7d12ba788a63",
   "metadata": {},
   "source": [
    "where `spikes` should be _s_-by-_c_-by-_t_, where _s_ is the number of stimulus presentations (rows) in the input dataframe, _c_ is the number of channels, and _t_ is the number of time bins. \n",
    "\n",
    "Note there is in general no guarantee that PSTH arrays from different rows of _rsvp44_data_ will have the same dimensions (e.g., different stimulus types may have different durations, and therefore different numbers of time bins), so it's possible the above line will fail. To guard for this explicitly, one can use a _try_ /_except_ statement like   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b1c5955-3358-452d-80bf-0cedfdb48c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    spikes = np.array(list(rsvp44_data['psth']))\n",
    "except ValueError:\n",
    "    raise ValueError('Different # channels or time bins found for different stimulus presentations; can''t combine into single array.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbf81a-4ac3-4e34-8b68-65b840f6c163",
   "metadata": {},
   "source": [
    "Because most neural data analyses depend on time, it's typically necessary to select spikes from a specific time interval (relative to stimulus onset or saccade, etc.). Moreover, it's common to repeat analyses over multiple time bins. Thus to avoid unnecessary disk read operations, it can be more efficient to load data spanning a relatively wide interval just once, then repeat your analysis over smaller sub-intervals by iterating over indices of `spikes`. This in turn typically requires knowing the mapping between array indices and bin start times relative to stimulus onset, which you can check by inspecting the `psth_bins` column of the `rsvp44_data` dataframe:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1bd57f6-ffaf-470e-b076-cf358d75c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "psth_bins = rsvp44_data['psth_bins']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280357f-89c0-4ec5-b560-9631f610a941",
   "metadata": {},
   "source": [
    "This returns an _s_-element pandas Series `psth_bins`, each element of which should in turn be a _t_-element array stating the spike bin start times (again, relative to stimulus onset) of the corresponding stimulus presentation.\n",
    "\n",
    "While different stimulus presentations may in principle have different spike bin edges, they are usually the same the vast majority of the time. We can verify this by doing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a822ddba-be0d-4160-88b1-f226fb653178",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Try to combine psth_bins from different stimulus presentations into a single array\n",
    "    P = np.array(list(psth_bins)) # s-by-t   \n",
    "except ValueError: # Raise exception if different stimulus presentations have different numbers of time bins\n",
    "    raise ValueError('Different # time bins found for different stimulus presentations; can''t combine into single array.')\n",
    "\n",
    "# Verify that all bin edges have the same value\n",
    "dP = np.ptp(P, axis=0)\n",
    "if np.all(dP==0):\n",
    "    uniform_bins = True\n",
    "else:\n",
    "    uniform_bins = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361754ba-2d37-4e2c-829d-9a874f064096",
   "metadata": {},
   "source": [
    "If it turns out that all stimulus presentations have the same spike bins, then we can just get the standard set of spike bins edges using,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "006ffed5-9a28-4ceb-98a5-8e65f00126f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = psth_bins.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2da20-4f8e-4a73-9199-e3fdc8c5fc6f",
   "metadata": {},
   "source": [
    "And then index into our `spikes` array as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6654d5-87c3-44a6-bec4-5abee4dd2b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired time interval rel. stim onset, in seconds:\n",
    "start_time = 0.06 \n",
    "stop_time = 0.12 \n",
    "\n",
    "# Convert bin edges to array indices:\n",
    "start_idx = max(np.argwhere(bin_edges <= start_time))[0]\n",
    "stop_idx = max(np.argwhere(bin_edges <= stop_time))[0]\n",
    "\n",
    "# Extract spike data for desired interval:\n",
    "spikes_t = spikes[:,:,start_idx:stop_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e105b-1908-4358-92dd-7c883615605b",
   "metadata": {},
   "source": [
    "If it turns out not to be the case that all stimulus presentations have the same spike bin edges, then indices will have to be looked up on a row-by-row basis within `rsvp44_data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63290461-a586-4b6f-8a96-5e59cf7caf7d",
   "metadata": {},
   "source": [
    "Last updated DDK 2025-04-08."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
