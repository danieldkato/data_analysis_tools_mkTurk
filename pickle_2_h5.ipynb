{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e85bcb-16f0-43d0-a94f-3da330d19317",
   "metadata": {},
   "source": [
    "# Working with HDF5\n",
    "This notebook illustrates how to work with MkTurk PSTH data in HDF5 format. HDF5s allow you to slice data stored on disk, precluding the need to load all of the data from a given session before working with it. In applications where only a subset of the data are needed (e.g., only trials from a certain scenefile), this can reduce read times several times over, especially if the selected slices are contiguous on disk.\n",
    "\n",
    "By way of preliminaries, let's load some necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e039a9-c6b9-4908-afff-311f780d4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from data_analysis_tools_mkTurk.utils_meta import get_recording_path\n",
    "from data_analysis_tools_mkTurk.general import df_2_psth_mat\n",
    "from data_analysis_tools_mkTurk.IO import ch_dicts_2_h5, h5_2_trial_df, h5_2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a9d8c8-508d-4354-b6fd-ea50c9e1b9db",
   "metadata": {},
   "source": [
    "Also, use the cell below to set the prefix used to access Engram on the local system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1309884c-918c-494a-9642-93ac4441ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'X:' # Edit this to whatever prefix is used to access Engram as a share/network drive on the local system \n",
    "mnt_path = prefix + os.path.sep \n",
    "base_data_path = os.path.join(mnt_path, 'Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fff41-856c-4e8c-8ed7-3edb9c8d4b5b",
   "metadata": {},
   "source": [
    "      \n",
    "<br/>\n",
    "\n",
    "## 1. Writing data to HDF5\n",
    "In this first part, we'll convert pickeld dicts of trial parameters and spike count data into HDF5 format, which we'll then read from in the subsequent section.   \n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "<u>**Prerequisites**</u><br/>\n",
    "This write portion of the pipeline assumes that ephys data from SpikeGLX and behavioral data from MkTurk have already been preprocessed and registered to each other using the code illustrated in notebooks `get_data_dict_from_mkturk.ipynb`, `Demo.ipynb`, `Demo_2.ipynb`, and `Demo3.ipynb`. This should result in the following files for any given session:\n",
    "\n",
    "- `data_dict_<session>`, where `<session>` stands for the name of the directory where the raw data for the session are saved.\n",
    "- `stim_info_sess`, a pickled dict of general information about the stimuli shown in the session.\n",
    "- `ch<iii>_psth_stim_meta`, a pickled dict including timing information about the peristimulus window for each stimulus, for each channels `<iii>`. \n",
    "- `ch<iii>_psth_stim`, a pickled dict of spike count data for each channels `<iii>`.\n",
    "\n",
    "For an example of what preprocessed data for a given session should look like, see `axon.rc.zi.columbia.edu/mnt/smb/locker/issa-locker/users/Dan/ephys/West/West_20231109_R_H00_P46`. Let's use that session as an example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b94800-909e-4b5d-be77-d7ff3d953cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\\Data\\West\n"
     ]
    }
   ],
   "source": [
    "monkey = 'West'\n",
    "date = '20231109'\n",
    "\n",
    "# Define output directory:\n",
    "recording_path = get_recording_path(base_data_path, monkey, date, depth=4)[0]\r",
    "preprocessed_data_path = os.path.join(mnt_path, 'users', 'Dan', 'ephys', monkey, recording_path.split(os.path.sep)[3])  \n",
    "output_directory = os.path.join(mnt_path, 'users', 'Dan', 'ephys', monkey, recording_path.split(os.path.sep)[3], 'hdf_demo', 'write')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ea57d-0b1a-4082-a3e2-6c09a60ab285",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<u>**Choosing chunk size**</u><br/>\n",
    "When working with HDF5s, data are loaded in \"chunks,\" or sets of contiguous memory addresses. The chunk size determines how many such memory addresses are loaded at once. Perhaps counterintiuitively, despite the fact that this parameter relates to read behavior, it has to be set when the file is written. **Choosing an appropriate chunk size is really important!** The difference between a good and a bad chunk size can be a >100x difference in read/write times! In the past a chunk size of 100 has worked pretty well for MkTurk data. Also, since spike counts are always integers, we can save a little time and storage space by specifying that the datatype should be `int`:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b227215c-4e10-45de-b529-f288a778af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose HDF5 params:\n",
    "chunk_size = 100\n",
    "dtype = int "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e848ed4-b005-48bf-9cd7-fe3a50e18720",
   "metadata": {},
   "source": [
    " <br/>\n",
    " <br/>\n",
    " \n",
    "<u>**Writing HDF5 to disk**</u><br/>\n",
    "We're almost ready to convert the preprocessed data to HDF5. First though, for demonstration/debugging purposes, we can also select a subset of channels to save data for, which will just make the demo much shorter. Let's just choose the first `max_channel` channels here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa47091-b202-4155-88ea-ccfc44be0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose channels to save data for:\n",
    "max_channel = 4\n",
    "channels = np.arange(max_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9820e08-83a9-4f53-9a76-ead0a84b2cf7",
   "metadata": {},
   "source": [
    "In this case the channels happen to be consecutively numbered, but note in principle it's possible to select any arbitrary array (or list) of channel indices. Now we're ready to convert the preprocessed data to HDF5 using the `ch_dicts_2_h5()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb0ebf3-ae98-4725-82f9-844b85d9bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\\Data\\West\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\users\\Dan\\code\\data_analysis_tools_mkTurk\\IO.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  trial_params_df['idx_merge'] = np.arange(n_rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataframe of stimulus conditions...\n",
      "X:\\Data\\West\n",
      "X:\\Data\\West\n",
      "Loading data for channel 1 of 4...\n",
      "Loading data for channel 2 of 4...\n",
      "Loading data for channel 3 of 4...\n",
      "Loading data for channel 4 of 4...\n",
      "Saving HDF5 to disk...\n",
      "... done.\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data as HDF5:\n",
    "start = time.time() \n",
    "ch_dicts_2_h5(base_data_path, monkey, date, preprocessed_data_path, channels=channels, chunk_size=chunk_size, dtype=dtype, \n",
    "    save_output=True, output_directory=output_directory)\n",
    "stop = time.time()\n",
    "print('... done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179b16e-ea69-4c6a-9b7f-c49594c0580a",
   "metadata": {},
   "source": [
    "The amount of time this step takes depends on where you're running the code/where you're writing to. If writing from Axon to Engram, saving an entire session should take under a minute; if saving from a laptop to Engram over Columbia private WiFi, about 5-12 minutes; over the public Internet (via VPN), upwards of an hour. This sounds like a lot, but the idea is that you only have to do it once (or at least rarely) and then it'll pay off in the longer run with all the time you save loading in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ca26bc-79a0-435b-bcb6-535cf4025235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write duration = 2.572272205352783 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Write duration = {} minutes'.format( (stop-start)/60 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60292d8-260e-4b51-bf4a-6446c3463055",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<u>**A caveat about time bins**</u></br>\n",
    "Note that HDF5 stores data in an array-like structure, meaning that PSTHs for all stimuli must be of uniform shape -- meaning in turn that they must all have the same number of time bins. However, also note that different stimuli within a session can be of different duration (e.g., RSVP44 vs freeview stimuli). To accommodate these differences, `ch_dicts_2_h5()` pads PSTHs for all stimuli to be equal in duration to the max PSTH duration across the whole session. In other words, the number of time bins for every stimulus is made to equal the number of time bins for the longest stimulus. Given a session where the maximum stim duration is _v_ time bins, any shorter stimulus _u_ bins in duration will be aligned to the _start_ of the peristimulus window so that only the first _u_ out of _v_ bins will include spike data. Any excess time bins will be all nan. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36fe31d-b47a-4152-bb0c-335c2864a513",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "# 2. Reading data from HDF5\n",
    "Now that the preprocessed data have been saved to an HDF5, we can try reading it into RAM. \n",
    "\n",
    "<br/>\n",
    "\n",
    "<u>**Pre-selecting trials**</u><br/>\n",
    "Recall one of the useful things about HDF5 is that we can retrieve specific slices of data from disk without having to load the entire file. In practice, this often means loading the data for a subset of trials. Suppose we only want trials from scenefiles `neural_stim_5_1ABC_00` and `neural_stim_5_2UVW_03`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e9aa0c-cd29-4643-a08a-ed7dea36f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_path = os.path.join(mnt_path, 'users', 'Dan', 'ephys', monkey, recording_path.split(os.path.sep)[3], 'hdf_demo', 'read', 'all_psth.h5')\n",
    "scenefiles = ['/mkturkfiles/scenebags/West/neural_stim_5_1ABC_00.json', \n",
    "              '/mkturkfiles/scenebags/West/neural_stim_5_2UVW_03.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b1cdd-296c-4a5c-94ab-332b0c653599",
   "metadata": {},
   "source": [
    "Note here we're loading from a previously-written HDF5 containing data for all channels, in order to demonstrate read times for a more realistically-sized dataset. In order to retrive just the responses to these scenefiles, we first need to know which trials/RSVP stim are associated with each one. To check this, we can first load just the trial parameters (without the actual PSTH data) using the function `h5_2_trial_df()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3b271d0-e51a-4c69-bd7b-60c5b2715739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\psth\\Lib\\site-packages\\tables\\attributeset.py:290: DataTypeWarning: Unsupported type for attribute 'scenefile_by_stim_mat' in node '/'. Offending HDF5 class: 8\n",
      "  value = self._g_getattr(self._v_node, name)\n"
     ]
    }
   ],
   "source": [
    "# Load trial parameters without PSTH data:\n",
    "trial_params = h5_2_trial_df(h5_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3f5d4-f430-4254-bd7e-b3ad29222d15",
   "metadata": {},
   "source": [
    "This will return a pandas dataframe `trial_params` with columns `trial_num`, `rsvp_num`, `stim_id`, and `scenefile`, where each row corresponds to an individual RSVP stimulus presentation. (We can also get a more complete dataframe of all trial parameters by passing `params='all'` to `h5_2_trial_df()`, although the default subset of trial parameters is sufficient for many applications). We can then search for rows (stimulus presentations) associated with our scenefiles of interest using the standard constructions for querying dataframes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95beed0b-855c-425c-90ff-1a405e0032ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stimulus presentations associated with requested scenefiles:\n",
    "filter = trial_params.scenefile.isin(scenefiles)\n",
    "rsvp44_trials = trial_params[filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45eb10-2e8b-4d12-9720-c89a6b0e0724",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<u>**Fetching spike data**</u><br/>\n",
    "Now that we know which trials we want spike counts for, we can use the `h5_2_df()` function along with our dataframe of selected trials to load PSTHs just for those trials. Suppose moreover that we just want data from a certain time window and from certain channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d46ff3ad-35d7-4771-a372-f52ac6a0b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-fetching PSTHs from HDF5...\n",
      "... done.\n",
      "Duration=3.738640081882477 minutes\n",
      "Fancy slicing numpy array...\n",
      "... done.\n",
      "Duration=0.007328236103057861 minutes\n"
     ]
    }
   ],
   "source": [
    "# Read spike count data from HDF5 for requested trials:\n",
    "time_window = [-0.1, 0.25] # Beginning and end of peristimulus time window for each stim, relative to trigger in seconds   \n",
    "channels = np.arange(max_channel-1) # Select a subset of channels for demo purposes\n",
    "rsvp44_data = h5_2_df(h5_path, trials=rsvp44_trials, time_window=time_window, channels=channels) # Fetch PSTHs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c0a43-9269-4fac-8695-a6c161b4cfcd",
   "metadata": {},
   "source": [
    "Note this new dataframe `rsvp44_data` includes an additional column `psth` containing actual binned spike count data. For each row, the value of `psth` is a _c_-by-_t_ numpy array of binned spike counts, where _c_ is the number of channels in the loaded data, and _t_ is the number of time bins in the requested peri-stimulus time window. <br/>\n",
    "\n",
    "Alternatively, `trials` can be an _s_-by-2 numpy array of trial/RSVP indices, where the first column consists of trial numbers and the second column consists of RSVP numbers within the corresponding trial. If `trials`, `time_window`, or `channels` are left unset, then all data for all trials, time bins, and channels will be returned, respectively.<br/>\n",
    "\n",
    "To extract just the spike data into a numpy array, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07575299-2dfe-410c-accb-e4758f75a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spike count data as array:\n",
    "spikes = df_2_psth_mat(rsvp44_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2937df7-0ad6-47c2-b906-7d12ba788a63",
   "metadata": {},
   "source": [
    "where `spikes` will be _c_-by-_t_-by-_s_, where _s_ is the number of individual stimulus presentations in the input dataframe (i.e number of rows). \n",
    "\n",
    "Last updated DDK 2024-03-13."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
